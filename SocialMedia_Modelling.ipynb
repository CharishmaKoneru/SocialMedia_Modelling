{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Import needed packages¶\n",
    "In [2]:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from googletrans import Translator\n",
    "load in Facebook, Twitter, Youtube Post/Video Performance file\n",
    "In [3]:\n",
    "df_twit = pd.read_excel('Tweet text.xlsx')\n",
    "df_fb = pd.read_excel('Facebook Text.xlsx')\n",
    "df_yt = pd.read_excel('Youtube Text.xlsx')\n",
    "In [32]:\n",
    "df_twit.head(0)\n",
    "Out[32]:\n",
    "Tweet id\tTweet permalink\tTweet text\ttime\timpressions\tengagements\tengagement rate\tretweets\treplies\tlikes\t...\tpermalink clicks\tapp opens\tapp installs\tfollows\temail tweet\tdial phone\tmedia views\tmedia engagements\tyear\tmonth\n",
    "0 rows × 24 columns\n",
    "\n",
    "In [33]:\n",
    "df_twit.tail(0)\n",
    "Out[33]:\n",
    "Tweet id\tTweet permalink\tTweet text\ttime\timpressions\tengagements\tengagement rate\tretweets\treplies\tlikes\t...\tpermalink clicks\tapp opens\tapp installs\tfollows\temail tweet\tdial phone\tmedia views\tmedia engagements\tyear\tmonth\n",
    "0 rows × 24 columns\n",
    "\n",
    "In [34]:\n",
    "df_fb.head(0)\n",
    "Out[34]:\n",
    "Permalink\tPost Message\tType\tPosted\tLifetime Post Total Reach\tLifetime Post organic reach\tLifetime Post Paid Reach\tLifetime Post Total Impressions\tLifetime Post Organic Impressions\tLifetime Post Paid Impressions\t...\tLifetime Post Paid Impressions by people who have liked your Page\tLifetime Paid reach of a post by people who like your Page\tLifetime People who have liked your Page and engaged with your post\tlike\tshare\tcomment\tother clicks\tlink clicks\tphoto view\tMonth\n",
    "0 rows × 24 columns\n",
    "\n",
    "In [35]:\n",
    "df_fb.tail(0)\n",
    "Out[35]:\n",
    "Permalink\tPost Message\tType\tPosted\tLifetime Post Total Reach\tLifetime Post organic reach\tLifetime Post Paid Reach\tLifetime Post Total Impressions\tLifetime Post Organic Impressions\tLifetime Post Paid Impressions\t...\tLifetime Post Paid Impressions by people who have liked your Page\tLifetime Paid reach of a post by people who like your Page\tLifetime People who have liked your Page and engaged with your post\tlike\tshare\tcomment\tother clicks\tlink clicks\tphoto view\tMonth\n",
    "0 rows × 24 columns\n",
    "\n",
    "In [36]:\n",
    "df_yt.head(0)\n",
    "Out[36]:\n",
    "video\tvideo_title\twatch_time_minutes\tviews\tsubscribers\tvideo_thumbnail_impressions\tvideo_thumbnail_impressions_ctr\n",
    "In [37]:\n",
    "df_yt.tail(1)\n",
    "Out[37]:\n",
    "video\tvideo_title\twatch_time_minutes\tviews\tsubscribers\tvideo_thumbnail_impressions\tvideo_thumbnail_impressions_ctr\n",
    "29\tTotal\tNaN\t36270.1927\t19790\t103\tNaN\tNaN\n",
    "Function 1: Topic Modelling for All Texts\n",
    "In [28]:\n",
    "def topic_model(file, colname, topics_num):\n",
    "    \n",
    "    import nltk\n",
    "    import pandas as pd\n",
    "    from nltk.corpus import stopwords\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "    if 'csv' in file:\n",
    "        df = pd.read_csv(file)\n",
    "    elif 'xlsx' in file:\n",
    "        df = pd.read_excel(file)\n",
    "    \n",
    "    # Clean the text\n",
    "    df = df.dropna(subset=[colname])\n",
    "    df['clean_title'] = df[colname].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    df['clean_title'] = df['clean_title'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "    df['clean_title'] = df['clean_title'].apply(lambda x: x.lower())\n",
    "    \n",
    "    # deal with the stop word\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenized_doc = df['clean_title'].apply(lambda x: x.split())\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    \n",
    "    # merge the tokenized word back to sentences again\n",
    "    detokenized_doc = []\n",
    "    for i in range(len(df)):\n",
    "        t = ' '.join(tokenized_doc[i])\n",
    "        detokenized_doc.append(t)\n",
    "    df['clean_title'] = detokenized_doc\n",
    "\n",
    "    # vectorize it\n",
    "    vectorizer = TfidfVectorizer(max_features= 500, # keep top 500 terms \n",
    "                                 max_df = 0.5, \n",
    "                                 smooth_idf=True)\n",
    "\n",
    "    X = vectorizer.fit_transform(df['clean_title'])\n",
    "    \n",
    "\n",
    "    # SVD represent documents and terms in vectors \n",
    "    svd_model = TruncatedSVD(n_components = topics_num, algorithm='randomized', n_iter=100, random_state=122)\n",
    "    svd_model.fit(X)\n",
    "    \n",
    "    terms = vectorizer.get_feature_names()\n",
    "\n",
    "    for i, comp in enumerate(svd_model.components_):\n",
    "        terms_comp = zip(terms, comp)\n",
    "        sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:6]\n",
    "        print(\"Topic \"+str(i)+\": \")\n",
    "        print('-------')\n",
    "        for t in sorted_terms:\n",
    "            print(t[0])\n",
    "            print(' ')\n",
    "Function 2: Topic Modelling for All Texts\n",
    "In [27]:\n",
    "def topic_model_quantile(file, colname, metric_col, lower_quantile_no, upper_quantile_no ,topics_num):\n",
    "    \n",
    "    import nltk\n",
    "    import pandas as pd\n",
    "    from nltk.corpus import stopwords\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "    if 'csv' in file:\n",
    "        df = pd.read_csv(file)\n",
    "    elif 'xlsx' in file:\n",
    "        df = pd.read_excel(file)\n",
    "    \n",
    "    df = df.dropna(subset=[colname])\n",
    "    \n",
    "    # Subset the data with quantile\n",
    "    lower_quantile, upper_quantile = df[metric_col].quantile([lower_quantile_no/100, upper_quantile_no/100])\n",
    "    df = df.loc[(df[metric_col] > lower_quantile) & (df[metric_col] < upper_quantile)]\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # Clean the text\n",
    "    df['clean_title'] = df[colname].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    df['clean_title'] = df['clean_title'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "    df['clean_title'] = df['clean_title'].apply(lambda x: x.lower())\n",
    "    \n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    tokenized_doc = df['clean_title'].apply(lambda x: x.split())\n",
    "\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    detokenized_doc = []\n",
    "    for i in range(len(df)):\n",
    "        t = ' '.join(tokenized_doc[i])\n",
    "        detokenized_doc.append(t)\n",
    "\n",
    "    df['clean_title'] = detokenized_doc\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features= 500, # keep top 1000 terms \n",
    "                                 max_df = 0.5, \n",
    "                                 smooth_idf=True)\n",
    "\n",
    "    X = vectorizer.fit_transform(df['clean_title'])\n",
    "    \n",
    "\n",
    "    # SVD represent documents and terms in vectors \n",
    "    svd_model = TruncatedSVD(n_components = topics_num, algorithm='randomized', n_iter=100, random_state=122)\n",
    "\n",
    "    svd_model.fit(X)\n",
    "    \n",
    "    terms = vectorizer.get_feature_names()\n",
    "\n",
    "    for i, comp in enumerate(svd_model.components_):\n",
    "        terms_comp = zip(terms, comp)\n",
    "        sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:]\n",
    "        print(\"Topic \"+str(i)+\": \")\n",
    "        print('-------')\n",
    "        for t in sorted_terms:\n",
    "            print(t[0])\n",
    "            print(' ')\n",
    "In [26]:\n",
    "topic_model('Youtube Text.xlsx','video_title',3)\n",
    "Topic 0: \n",
    "-------\n",
    "injuries\n",
    " \n",
    "needlestick\n",
    " \n",
    "preventing\n",
    " \n",
    "proper\n",
    " \n",
    "farms\n",
    " \n",
    "Topic 1: \n",
    "-------\n",
    "agujas\n",
    " \n",
    "apropiado\n",
    " \n",
    "lesiones\n",
    " \n",
    "piquetes\n",
    " \n",
    "previniendo\n",
    " \n",
    "Topic 2: \n",
    "-------\n",
    "health\n",
    " \n",
    "safety\n",
    " \n",
    "center\n",
    " \n",
    "midwest\n",
    " \n",
    "umash\n",
    " \n",
    "In [21]:\n",
    "topic_model('Tweet text.xlsx', 'Tweet text',5)\n",
    "Topic 0: \n",
    "safety\n",
    " \n",
    "farm\n",
    " \n",
    "health\n",
    " \n",
    "check\n",
    " \n",
    "safe\n",
    " \n",
    "Topic 1: \n",
    "safe\n",
    " \n",
    "thanks\n",
    " \n",
    "farm\n",
    " \n",
    "stay\n",
    " \n",
    "sharing\n",
    " \n",
    "Topic 2: \n",
    "thanks\n",
    " \n",
    "safe\n",
    " \n",
    "sharing\n",
    " \n",
    "health\n",
    " \n",
    "stay\n",
    " \n",
    "Topic 3: \n",
    "usagcenters\n",
    " \n",
    "nfshw\n",
    " \n",
    "safety\n",
    " \n",
    "thanks\n",
    " \n",
    "necasag\n",
    " \n",
    "Topic 4: \n",
    "usagcenters\n",
    " \n",
    "webinar\n",
    " \n",
    "work\n",
    " \n",
    "asap\n",
    " \n",
    "agriculture\n",
    " \n",
    "In [29]:\n",
    "topic_model_quantile('Tweet text.xlsx', 'Tweet text', 'engagements', 75, 100 , 3)\n",
    "Topic 0: \n",
    "-------\n",
    "safety\n",
    " \n",
    "farm\n",
    " \n",
    "check\n",
    " \n",
    "farmsafety\n",
    " \n",
    "stress\n",
    " \n",
    "month\n",
    " \n",
    "Topic 1: \n",
    "-------\n",
    "stress\n",
    " \n",
    "women\n",
    " \n",
    "resiliency\n",
    " \n",
    "agriculture\n",
    " \n",
    "find\n",
    " \n",
    "cultivating\n",
    " \n",
    "Topic 2: \n",
    "-------\n",
    "grain\n",
    " \n",
    "week\n",
    " \n",
    "safety\n",
    " \n",
    "rescue\n",
    " \n",
    "women\n",
    " \n",
    "open\n",
    " \n",
    "In [31]:\n",
    "topic_model_quantile('Facebook Text.xlsx', 'Post Message', 'Lifetime Post Total Impressions', 0, 40, 4)\n",
    "Topic 0: \n",
    "-------\n",
    "safety\n",
    " \n",
    "health\n",
    " \n",
    "upper\n",
    " \n",
    "center\n",
    " \n",
    "midwest\n",
    " \n",
    "agricultural\n",
    " \n",
    "Topic 1: \n",
    "-------\n",
    "farm\n",
    " \n",
    "umash\n",
    " \n",
    "http\n",
    " \n",
    "partner\n",
    " \n",
    "check\n",
    " \n",
    "safety\n",
    " \n",
    "Topic 2: \n",
    "-------\n",
    "umash\n",
    " \n",
    "field\n",
    " \n",
    "catch\n",
    " \n",
    "partner\n",
    " \n",
    "activities\n",
    " \n",
    "conference\n",
    " \n",
    "Topic 3: \n",
    "-------\n",
    "umash\n",
    " \n",
    "field\n",
    " \n",
    "http\n",
    " \n",
    "great\n",
    " \n",
    "news\n",
    " \n",
    "story\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
